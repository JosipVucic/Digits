{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NdWzMFPgXq3k"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GACNN Model Class"
      ],
      "metadata": {
        "id": "2L_PR5uFXleM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wW-eQnPCWcag"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import List\n",
        "\n",
        "# default genotype, 13 tuples, one tuple defines one block of the network\n",
        "# (conv_output_channels, pool_layer_present, conv_kernel_size)\n",
        "DEFAULT_GENOTYPE = [\n",
        "    (57, 0, 3),\n",
        "    (96, 0, 1),\n",
        "    (36, 0, 1),\n",
        "    (104, 0, 1),\n",
        "    (97, 0, 3),\n",
        "    (18, 0, 1),\n",
        "    (60, 0, 3),\n",
        "    (66, 1, 3),\n",
        "    (82, 0, 3),\n",
        "    (94, 0, 3),\n",
        "    (210, 1, 3),\n",
        "    (17, 1, 1),\n",
        "    (100, 0, 3)]\n",
        "\n",
        "\n",
        "class GACNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture consisting of 13 blocks, defined by genotype found using genetic algorithm evolution.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            genotype=None,\n",
        "            num_classes: int = 10,\n",
        "            in_chans: int = 1,\n",
        "\n",
        "    ):\n",
        "        \"\"\"Instantiates a GACNN model comprised of 13 blocks, each block is defined by number of convolutional output\n",
        "        channels, presence of a pooling layer, and kernel size for the convolution\n",
        "\n",
        "        Args:\n",
        "            genotype (list, optional): list of 13 3tuples defining the model architecture\n",
        "            num_classes (int, optional): number of classes. Defaults to 10.\n",
        "            in_chans (int, optional): number of input channels. Defaults to 1.\n",
        "            \"\"\"\n",
        "        super(GACNN, self).__init__()\n",
        "\n",
        "        if genotype is None:\n",
        "            genotype = DEFAULT_GENOTYPE\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.in_chans = in_chans\n",
        "\n",
        "        self.features = self._make_layers(genotype)\n",
        "        self.classifier = nn.Linear(round(genotype[-1][0]), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The input is processed by the model body, then a global pooling operation processes the feature maps changing\n",
        "        their size to 1x1, they get concatenated into a feature vector which enters the linear classifier.\n",
        "\n",
        "        :param x: model input\n",
        "        \"\"\"\n",
        "        out = self.features(x)\n",
        "        out = F.max_pool2d(out, kernel_size=out.size()[2:])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, genotype):\n",
        "        \"\"\"\n",
        "        Makes the model according to the provided genotype.\n",
        "\n",
        "        :param genotype: model genotype - list containing 13 3tuples of parameters\n",
        "        \"\"\"\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        input_channel = self.in_chans\n",
        "        for idx, (layer, pool, kernel_size) in enumerate(\n",
        "                genotype\n",
        "        ):\n",
        "            if pool == 1:\n",
        "                layers += [\n",
        "                    nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
        "                ]\n",
        "            else:\n",
        "                filters = round(layer)\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(input_channel, filters, kernel_size=kernel_size, stride=1, padding=1),\n",
        "                    nn.BatchNorm2d(filters, eps=1e-05, momentum=0.05, affine=True),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ]\n",
        "\n",
        "                input_channel = filters\n",
        "\n",
        "        model = nn.Sequential(*layers)\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain(\"relu\"))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train function"
      ],
      "metadata": {
        "id": "NdWzMFPgXq3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(genotype=None, batch_size=32, epoch_limit=1000):\n",
        "    \"\"\"\n",
        "    Loads MNIST dataset. Trains the model defined by genotype on MNIST for a number of epochs.\n",
        "    Training will stop after epoch limit is reached or after the validation accuracy stops rising.\n",
        "\n",
        "    :param genotype: Genotype for GACNN model\n",
        "    :param batch_size: Batch size for data loader\n",
        "    :param epoch_limit: Maximum number of epochs to train model\n",
        "    \"\"\"\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    trainset, validationset = random_split(trainset, [0.8, 0.2])\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    num_classes = 10\n",
        "    in_chans = 1\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    validationloader = DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = GACNN(genotype=genotype, num_classes=num_classes, in_chans=in_chans)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    acc_best, epoch_best, epoch, acc, loss = (0, 0, 0, 0, 0)\n",
        "\n",
        "    t0 = time.time()\n",
        "    while (acc >= acc_best or epoch - epoch_best < 5) and epoch < epoch_limit:\n",
        "\n",
        "        for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        acc = evaluate_model(model, validationloader)\n",
        "\n",
        "        if acc > acc_best:\n",
        "            acc_best, epoch_best = (acc, epoch)\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "        print(f\"Epoch {epoch} - loss = {loss} | validation accuracy = {acc}\")\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    train_acc = evaluate_model(model, trainloader)\n",
        "    test_acc = evaluate_model(model, testloader)\n",
        "    print(f\"Epoch {epoch} - train accuracy = {train_acc} | test accuracy = {test_acc}\")\n",
        "    print(f\"Training time: {t1 - t0}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, testloader):\n",
        "    \"\"\"\n",
        "    Evaluates model on given dataloader.\n",
        "\n",
        "    :param model: PyTorch model to evaluate\n",
        "    :param testloader: DataLoader object with evaluation dataset\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total\n"
      ],
      "metadata": {
        "id": "RKxjVrWkXuly"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and saving the model"
      ],
      "metadata": {
        "id": "fpGdvpj_X1Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()\n",
        "torch.save(model.state_dict(), 'default.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_T_fM59X0YF",
        "outputId": "452b0a82-8ecf-488c-80a2-29e6f1a18a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 53.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - loss = 0.02643842063844204 | validation accuracy = 98.975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:28<00:00, 53.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - loss = 0.003444208763539791 | validation accuracy = 98.725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - loss = 0.016698498278856277 | validation accuracy = 98.94166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:28<00:00, 52.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - loss = 0.01131428312510252 | validation accuracy = 98.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 53.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - loss = 0.008561301045119762 | validation accuracy = 99.06666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 53.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - loss = 0.10798192024230957 | validation accuracy = 99.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - loss = 0.00010499104973860085 | validation accuracy = 99.25833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - loss = 0.00042891831253655255 | validation accuracy = 99.19166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - loss = 0.0015849834308028221 | validation accuracy = 99.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - loss = 0.0001247662876266986 | validation accuracy = 99.03333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - loss = 0.00028661233955062926 | validation accuracy = 99.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - loss = 0.06826110184192657 | validation accuracy = 99.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:27<00:00, 54.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - loss = 0.0008903602720238268 | validation accuracy = 99.35\n",
            "Epoch 13 - train accuracy = 99.86875 | test accuracy = 99.5\n",
            "Training time: 414.4551966190338\n"
          ]
        }
      ]
    }
  ]
}